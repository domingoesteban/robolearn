{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Values\n",
    "- Rank: Number of dimensions\n",
    "- Shape: Tuple specifying the array's length along each dimension\n",
    "- Example:\n",
    "  + 3. --> Rank:0 (scalar) | Shape:[]\n",
    "  + [1., 2., 3.]  --> Rank:1 (vector) | Shape: [3]\n",
    "  + [[1., 2., 3.], [4., 5., 6]]  --> Rank:2 (matrix) | Shape: [2, 3]\n",
    "  + [[[1., 2., 3.]], [[4., 5., 6.]]]  --> Rank: 3  | Shape: [2, 1, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps:\n",
    "1. Build the computational graph: `tf.Graph`\n",
    "2. Run the computational graph: `tf.Session`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Graph\n",
    "- It is a series of TF operations arranged into a graph.\n",
    "- It is composed of 2 types of objects:\n",
    "  - Operations (ops): The nodes of the graph. Describe calculations that consume and produce tensors.\n",
    "  - Tensors: The edges of the graph. Values that will flow through the graph. Most TF fcns return `tf.Tensors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=float32)\nTensor(\"Const_1:0\", shape=(), dtype=float32)\nTensor(\"add:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()  # Reset the default graph\n",
    "a = tf.constant(3.0, dtype=tf.float32)\n",
    "b = tf.constant(4.0)  # also tf.float32 implicitly\n",
    "total = a + b\n",
    "print(a)\n",
    "print(b)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard\n",
    "- The event file from a graph has the format: `events.out.tfevents.{timestamp}.{hostname}`\n",
    "- Launch Tensorflow:\n",
    "``tensorboard --logdir {file_path}``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the computation graph to a TensorBoard summary file:\n",
    "writer = tf.summary.FileWriter('.')\n",
    "writer.add_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session\n",
    "- It encapsulates the state of the TF runtime, and runs TF ops. If a `tf.Graph` is like a `.py` file, a `tf.Session` is like the `python` executable.\n",
    "- Backtracks through the graph and runs all the nodes that provide input to the requested output(s) node(s).\n",
    "- During a call to `tf.Session.run` any `tf.Tensor` only has a single value.\n",
    "- Calling `run` on an Operations is `None` and it is done to cause a side-effect. E.g. initialization and training ops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0\n{'ab': (3.0, 4.0), 'total': 7.0}\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run(total))\n",
    "print(sess.run({'ab': (a, b), 'total': total}))  # Pass multiple tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.61810064 0.84265256 0.37434506]\n[0.99167466 0.59722066 0.09075904]\n(array([1.7061679, 1.347009 , 1.6094939], dtype=float32), array([2.706168 , 2.347009 , 2.6094937], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "# Show that during a call to `tf.Session.run` any `tf.Tensor` only has a single value.\n",
    "vec = tf.random_uniform(shape=(3,))\n",
    "out1 = vec + 1\n",
    "out2 = vec + 2\n",
    "print(sess.run(vec))\n",
    "print(sess.run(vec))\n",
    "print(sess.run((out1, out2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeding\n",
    "- A graph is parameterized to accept external inputs by placeholders.\n",
    "- A **placeholder** is a promise to provide a value later, like a fcn argument.\n",
    "- We use `feed_dict`  argument of the run method to feed concrete values to the placeholders.\n",
    "- `feed_dict argument can be used to overwrite any tensor in the graph. The difference with `tf.Tensors` is that placeholders throw an error if no values is fed to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5\n[3. 7.]\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "z = x + y\n",
    "print(sess.run(z, feed_dict={x: 3, y: 4.5}))\n",
    "print(sess.run(z, feed_dict={x: [1, 3], y: [2, 4]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "- Datasets are the preferred method of streaming data into a model\n",
    "- To get a runnable `tf.Tensor` from a Datset, first it should be converted to a `tf.data.Iterator`, and then call the method `get_next`\n",
    "- Reaching the end of the data stream causes Dataset to throw an `OutOfRangeError`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = [\n",
    "    [0, 1,],\n",
    "    [2, 3,],\n",
    "    [4, 5,],\n",
    "    [6, 7,],\n",
    "]\n",
    "slices = tf.data.Dataset.from_tensor_slices(my_data)\n",
    "next_item = slices.make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n[2 3]\n[4 5]\n[6 7]\n"
     ]
    }
   ],
   "source": [
    "# Reaching the end of the data stream causes Dataset to throw an \n",
    "# OutOfRangeError\n",
    "while True:\n",
    "    try:\n",
    "        print(sess.run(next_item))\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.13192289 1.602257   0.7300845 ]\n[-1.53792     0.22738095  0.8293629 ]\n[0.99698305 1.0610259  1.0443759 ]\n[-0.04267362  0.7620447  -1.7718678 ]\n[-0.8684591  -0.40656278 -1.0453751 ]\n[-1.0305398e+00 -1.5117089e+00  1.5716092e-04]\n[-0.3321954  1.0547798  0.9447274]\n[-1.5558867  -0.39574862  1.8517487 ]\n[ 1.0624346  -2.6477983  -0.07072894]\n[ 0.49216872  0.6381449  -1.0255786 ]\n"
     ]
    }
   ],
   "source": [
    "# If the Dataset depends on stateful operations you may need to \n",
    "# initialize the iterator before using it\n",
    "r = tf.random_normal([10, 3])\n",
    "dataset = tf.data.Dataset.from_tensor_slices(r)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_row = iterator.get_next()\n",
    "\n",
    "sess.run(iterator.initializer)\n",
    "while True:\n",
    "    try:\n",
    "        print(sess.run(next_row))\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers\n",
    "- A trainable model must modify the values in the graph to get new outputs with the same input. Layers are the preferred way to add trainable parameters to a graph.\n",
    "- Layers package together both the variables and the operations that act on them. For example a densely-connected layer performs a weighted sum across all inputs for each output and applies an optional activation function. The connection weights and biases are managed by the layer object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Layers\n",
    "To apply a layer to an input, call the layer as if it were a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "linear_model = tf.layers.Dense(units=1)\n",
    "y = linear_model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Layers\n",
    "- The layer contains variables that must be initialized before they can be used. \n",
    "- While it is possible to initialize variables individually, you can easily initialize all the variables in a TensorFlow graph as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.5302558]\n [1.0715563]]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(y, feed_dict={x: [[1, 2, 3], [4, 5, 6]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer function shortcuts\n",
    "- For each layer class (like `tf.layers.Dense`) TensorFlow also supplies a shortcut function (like `tf.layers.dense`). The only difference is that the shortcut function versions create and run the layer in a single call.\n",
    "- The problem of the above is that it makes introspection and debugging more difficult, and layer reuse impossible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.63285863]\n [ 0.15861702]]\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "y = tf.layers.dense(x, units=1)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "print(sess.run(y, {x: [[1, 2, 3], [4, 5, 6]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VARIABLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A `tf.Variable` represents a tensor whose value can be changed by running ops on it. Unlike `tf.Tensor` objects, it **exists outside** the context of a single `session.run` call.\n",
    "- A `tf.Variable` stores a persistent tensor. Specific ops allow you to read and modify the values of this tensor. These modifications are visible across multiple `tf.Session`s, so multiple workers can see the same values for a `tf.Variable`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Variable\n",
    "- `tf.get_variable` is the best way to create a variable.\n",
    "- It requires to specify a Variable's name that will be used by other replicas to access the same variable.\n",
    "- It allows you to reuse a previously created variable of the same name, making it easy to define models which reuse layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This variable has dtype tf.float32 and \n",
    "# is initialized by tf.glorot_uniform_initializer\n",
    "my_variable = tf.get_variable(\"my_variable\", [1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the dtype and initializer\n",
    "my_int_variable = tf.get_variable(\"my_int_variable\",\n",
    "                                  [1, 2, 3],\n",
    "                                  dtype=tf.int32,\n",
    "                                  initializer=tf.zeros_initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a variable to have the value of a Tf.Tensor\n",
    "other_variable = tf.get_variable(\"other_variable\",\n",
    "                                 dtype=tf.int32,\n",
    "                                 initializer=tf.constant([23, 42]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Collections\n",
    "- A **collection** is a named list of tensors or other variables, such as `tf.Variable` instances. It is useful to access easily the variables.\n",
    "- By default, every `tf.Variable` gets placed in the following 2 collections:\n",
    "  - `tf.GraphKeys.GLOBAL_VARIABLES`: Variables that can be shared across multiple devices.\n",
    "  - `tf.GraphKeys.TRAINABLE_VARIABLES`: Variables for which TF will calculate gradients.\n",
    "- If you don't waht a variable to be trainable, add it to the `tf.GraphKeys.LOCAL_VARIABLES` collection instead or use argument `trainable=False` in `tf.get_variable`.\n",
    "- It is possible to create your own collections by: `tf.add_to_collection` and to retrieve a list of all the variables (or other objects) with `tf.get_collection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This variables are not trainable, then gradients are not computed\n",
    "my_local = tf.get_variable(\"my_local\",\n",
    "                           shape=(),\n",
    "                           collections=[tf.GraphKeys.LOCAL_VARIABLES])\n",
    "my_non_trainable = tf.get_variable(\"my_non_trainable\",\n",
    "                                   shape=(),\n",
    "                                   trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'my_local:0' shape=() dtype=float32_ref>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a collection\n",
    "tf.add_to_collection('my_collection_name', my_local)\n",
    "\n",
    "# Retrieve all the variables (or other objects) in the collection\n",
    "tf.get_collection('my_collection_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device Placement\n",
    "- Like any other TF op, variables can be placed on particular devices\n",
    "- It is important to put variables on parameter servers and NOT on workers, the opposite can severely slow down raining or, in the worst case, let each worker blithely forge ahead with its own independent copy of each variable. \n",
    "- To avoid the aforementioned problem, `tf.train.replica_device_setter` can automatically place variables in parameter servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put variable in GPU device\n",
    "with tf.device(\"/device:GPU:1\"):\n",
    "    v = tf.get_variable('v', [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use replica_device_setter to place variables in parameter servers.\n",
    "cluster_spec = {\n",
    "    'ps': ['ps0:2222', 'ps1:2222'],\n",
    "    'worker': ['worker0:2222', 'worker1:2222', 'worker2:2222']\n",
    "}\n",
    "# This variables is placed in the parameter server by the replica_device_setter\n",
    "with tf.device(tf.train.replica_device_setter(cluster=cluster_spec)):\n",
    "    v = tf.get_variable('v', shape=[20, 20])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing variables\n",
    "- Before you can use a variable, it must be initialized.\n",
    "- Explicit initialization (that is without high-level TF API) allows you not to rerun expensive initializers when reloading a model from a checkpoint as well as determinism when randomly-initialized variables are shared in a distrubuted setting.\n",
    "- To initialize all trainable variables in `tf.GraphKeys.GLOBAL_VARIABLES` collection: `tf.global_variables_initializer()` \n",
    "- To initialize a variable: `session.run(my_variable.initializer)`\n",
    "- Print the names of all variables which have not yet been initialized: `tf.report_uninitialized_variables()`\n",
    "- NOTE: `tf.global_variables_initializer` does not specify the order in which variables are initialized. Then if the initial value of one variable depends on another variable's value, you can get an error. For this reason is better to use `variable.initialized_value()` instead of `variable` in the `initializer` parameter of `tf.get_variable`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "# Initialize all variables in tf.GraphKeys.GLOBAL_VARIABLES collection\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a variable yourself\n",
    "session.run(my_variable.initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'my_local']\n"
     ]
    }
   ],
   "source": [
    "# Ask which variables have still not been initialized\n",
    "print(session.run(tf.report_uninitialized_variables()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use of variable.initialized_value() if requires de value of other variable\n",
    "v = tf.get_variable('v', shape=(), initializer=tf.zeros_initializer())\n",
    "w = tf.get_variable('w', initializer=v.initialized_value() + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using variables\n",
    "- To use the value of a `tf.Variable` in a TF graph, treat it like a normal `tf.Tensor`\n",
    "- To assign a value to a variable, use methods: `assign`, `assign_add` in the `tf.Variable` class.\n",
    "- TF optimizer (see `tf.train.Optimizer`), update efficiently the values of variables accourding to some gradient descent-like algorithm.\n",
    "- To force a re-read of the value of a variable after something has happended, you can use: `tf.Variable.read_value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "v = tf.get_variable('v', shape=(), initializer=tf.zeros_initializer())\n",
    "# w is a tf.Tensor which is computed based on the value of v.\n",
    "# Any time a variable is used in an expression it gets automatically\n",
    "# converted to a tf.Tensor representing its value.\n",
    "w = v + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "# Assign a value to a variable\n",
    "v = tf.get_variable('v', shape=(), initializer=tf.zeros_initializer())\n",
    "assignment = v.assign_add(1)\n",
    "sess.run(tf.global_variables_initializer())  # or tf.global_variables_initializer().run()\n",
    "sess.run(assignment)  # or assignment.op.run(), or assignment.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force to re-read the value of a variable after something has happened.\n",
    "tf.reset_default_graph()\n",
    "v = tf.get_variable('v', shape=(), initializer=tf.zeros_initializer())\n",
    "assignment = v.assign_add(1)\n",
    "with tf.control_dependencies([assignment]):\n",
    "    w = v.read_value()  # w is guaranteed to reflect v's value after the assign_add op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sharing variables\n",
    "- TF supports 2 ways of sharing variables:\n",
    "  - Explicitly passing `tf.Variable` objects around.\n",
    "  - Implicitly wrapping `tf.Variable` objects within `tf.variable_scope` objects\n",
    "- While code which explicitly passes variables around is very clear, it is sometimes convenient to write TF functions that implicitly use variables in their implementations. E.g. the functional layers form `tf.layer`, `tf.metrics`.\n",
    "- Variables scopes allow you to control variable reuse when calling functions which implicitly create and use variables. They also allow you to name your variables in a hierarchical and understandable way.\n",
    "- If you want to variables to be **shared**: 1) Create it them with `reuse=True`. or 2)call `scope.reuse_variables()` to trigger a reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_relu(input, kernel_shape, bias_shape):\n",
    "    # Create variable named 'weights'\n",
    "    weights = tf.get_variable('weights', kernel_shape,\n",
    "                              initializer=tf.random_normal_initializer())\n",
    "    # Create variable named 'biases'\n",
    "    biases = tf.get_variable('biases', bias_shape,\n",
    "                             initializer=tf.constant_initializer(0.0))\n",
    "    conv = tf.nn.conv2d(input, weights,\n",
    "                        strides=[1, 1, 1, 1], padding='SAME')\n",
    "    return tf.nn.relu(conv + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# We cannot exploit the names weights and biases if we call the fcn\n",
    "# conv_relu many times\n",
    "input1 = tf.random_normal([1, 10, 10, 32])\n",
    "input2 = tf.random_normal([1, 20, 20, 32])\n",
    "x = conv_relu(input1, kernel_shape=[5, 5, 32, 32], bias_shape=[32])\n",
    "# THIS WILL FAIL:\n",
    "# x = conv_relu(x, kernel_shape=[5, 5, 32, 32], bias_shape=[32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By cally conv_relu in different scopes, we can clarify that we want to create new variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_image_filter(input_images):\n",
    "    with tf.variable_scope('conv1'):\n",
    "        # Variables created here will be named 'conv1/weights', 'conv1/biases'\n",
    "        relu1 = conv_relu(input_images, [5, 5, 32, 32], [32])\n",
    "    with tf.variable_scope('conv2'):\n",
    "        # Variables created here will be named 'conv2/weights', 'conv2/biases'\n",
    "        return conv_relu(relu1, [5, 5, 32, 32], [32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse a variable with reuse=True in scope\n",
    "with tf.variable_scope('model'):\n",
    "    output1 = my_image_filter(input1)\n",
    "with tf.variable_scope('model', reuse=True):\n",
    "    output2 = my_image_filter(input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse a variable with scope.reuse_variables()\n",
    "with tf.variable_scope('model') as scope:\n",
    "    output1 = my_image_filter(input1)\n",
    "    scope.reuse_variables()\n",
    "    output2 = my_image_filter(output1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since depending on exact string names of scopes can feel dangerous, \n",
    "# it's also possible to initialize a variable scope based on another one:\n",
    "with tf.variable_scope('model') as scope:\n",
    "    output1 = my_image_filter(input1)\n",
    "with tf.variable_scope(scope, reuse=True):\n",
    "    output2 = my_image_filter(input2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
