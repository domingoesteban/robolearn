{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center><u>IU-WeightedSAC -- Centauro Tray Environment</u></center></h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import importlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib nbagg\n",
    "# %matplotlib notebook\n",
    "\n",
    "# from robolearn.envs.simple_envs.goal_composition import GoalCompositionEnv\n",
    "# from robolearn_gym_envs.pybullet.manipulator2d3dof import Pusher2D3DofGoalCompoEnv\n",
    "from robolearn_gym_envs.pybullet.centauro import CentauroTrayEnv\n",
    "from robolearn.envs.normalized_box_env import NormalizedBoxEnv\n",
    "\n",
    "from robolearn.torch.models import NNQFunction, NNVFunction\n",
    "from robolearn.torch.models import NNMultiQFunction, NNMultiVFunction\n",
    "\n",
    "from robolearn.torch.policies import TanhGaussianWeightedMultiPolicy\n",
    "\n",
    "from robolearn.torch.rl_algos.sac.iu_weightedmultisac import IUWeightedMultiSAC\n",
    "\n",
    "from robolearn.utils.data_management import MultiGoalReplayBuffer\n",
    "\n",
    "from robolearn.utils.launchers.launcher_util import setup_logger\n",
    "import robolearn.torch.pytorch_util as ptu\n",
    "from robolearn.core import logger\n",
    "from robolearn.utils.plots import get_csv_data\n",
    "from robolearn.utils.plots import subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_environment(goal, tgt_pose, sim_timestep=0.01, frame_skip=1, seed=10):\n",
    "    \n",
    "    env_params = dict(\n",
    "        is_render=False,\n",
    "        obs_with_img=False,\n",
    "        goal_poses=[goal, (goal[0], 'any'), ('any', goal[1])],\n",
    "        rdn_goal_pose=True,\n",
    "        tgt_pose=tgt_pose,\n",
    "        rdn_tgt_object_pose=True,\n",
    "        sim_timestep=sim_timestep,\n",
    "        frame_skip=frame_skip,\n",
    "        obs_distances=True,\n",
    "        tgt_cost_weight=1.0,\n",
    "        goal_cost_weight=1.5,\n",
    "        ctrl_cost_weight=1.0e-4,\n",
    "        use_log_distances=True,\n",
    "        log_alpha=1.e-1,\n",
    "        max_time=None,\n",
    "        subtask=None,\n",
    "        seed=seed,\n",
    "    )\n",
    "    \n",
    "    # Environment\n",
    "    env = CentauroTrayEnv(\n",
    "        **env_params,\n",
    "    )\n",
    "\n",
    "    # Normalize environment\n",
    "    env = NormalizedBoxEnv(\n",
    "        env,\n",
    "        normalize_obs=False,\n",
    "        online_normalization=False,\n",
    "        obs_mean=None,\n",
    "        obs_var=None,\n",
    "        obs_alpha=0.001,\n",
    "    )\n",
    "\n",
    "    # Visualize costs\n",
    "#     env.reset()\n",
    "#     env.render()\n",
    "#     env.close()\n",
    "    return env, env_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_value_fcns(net_size=32, n_hidden=2, n_shared_hidden=0):\n",
    "    # --------------------- #\n",
    "    # Value Function Models #\n",
    "    # --------------------- #\n",
    "    # Unintentional Value functions --> Environment sub-goals (sub-tasks)\n",
    "    u_qf = NNMultiQFunction(obs_dim=obs_dim,\n",
    "                           action_dim=action_dim,\n",
    "                           n_qs=n_unintentions,\n",
    "                           shared_hidden_sizes=[net_size for _ in range(n_shared_hidden)],\n",
    "                           unshared_hidden_sizes=[net_size for _ in range(n_hidden)],\n",
    "                           )\n",
    "\n",
    "    u_qf2 = NNMultiQFunction(obs_dim=obs_dim,\n",
    "                            action_dim=action_dim,\n",
    "                            n_qs=n_unintentions,\n",
    "                            shared_hidden_sizes=[net_size for _ in range(n_shared_hidden)],\n",
    "                            unshared_hidden_sizes=[net_size for _ in range(n_hidden)],\n",
    "                            )\n",
    "    u_vf = NNMultiVFunction(obs_dim=obs_dim,\n",
    "                           n_vs=n_unintentions,\n",
    "                           shared_hidden_sizes=[net_size for _ in range(n_shared_hidden)],\n",
    "                           unshared_hidden_sizes=[net_size for _ in range(n_hidden)],\n",
    "                           )\n",
    "\n",
    "    # Intentional Value function --> Environment goal (full task)\n",
    "    i_qf = NNQFunction(obs_dim=obs_dim,\n",
    "                      action_dim=action_dim,\n",
    "                      hidden_sizes=[net_size for _ in range(n_hidden)])\n",
    "\n",
    "    i_qf2 = NNQFunction(obs_dim=obs_dim,\n",
    "                       action_dim=action_dim,\n",
    "                       hidden_sizes=[net_size for _ in range(n_hidden)])\n",
    "\n",
    "    i_vf = NNVFunction(obs_dim=obs_dim,\n",
    "                      hidden_sizes=[net_size for _ in range(n_hidden)])\n",
    "    \n",
    "    return u_qf, u_qf2, u_vf, i_qf, i_qf2, i_vf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_policy(\n",
    "    net_size=32,\n",
    "    n_shared_layers=0,\n",
    "    n_unshared_layers=2,\n",
    "    n_mix_layers=2,\n",
    "    shared_norm=False,\n",
    "    unshared_norm=False,\n",
    "    mix_norm=False,\n",
    "):\n",
    "    # ------ #\n",
    "    # Policy #\n",
    "    # ------ #\n",
    "    policy = TanhGaussianWeightedMultiPolicy(\n",
    "        obs_dim=obs_dim,\n",
    "        action_dim=action_dim,\n",
    "        n_policies=n_unintentions,\n",
    "        shared_hidden_sizes=[net_size for _ in range(n_shared_layers)],\n",
    "        unshared_hidden_sizes=[net_size for _ in range(n_unshared_layers)],\n",
    "        unshared_mix_hidden_sizes=[net_size for _ in range(n_mix_layers)],\n",
    "        stds=None,\n",
    "        shared_layer_norm=shared_norm,\n",
    "        unshared_layer_norm=unshared_norm,\n",
    "        mixture_layer_norm=mix_norm,\n",
    "        mixing_temperature=1.,\n",
    "        reparameterize=True,\n",
    "    )\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_replay_buffer(env, replay_buffer_size):\n",
    "    n_unintentions = env.n_subgoals\n",
    "    obs_dim = np.prod(env.observation_space.shape)\n",
    "    action_dim = np.prod(env.action_space.shape)\n",
    "    \n",
    "    # Replay Buffer\n",
    "    replay_buffer = MultiGoalReplayBuffer(\n",
    "        max_replay_buffer_size=replay_buffer_size,\n",
    "        obs_dim=obs_dim,\n",
    "        action_dim=action_dim,\n",
    "        reward_vector_size=n_unintentions,\n",
    "    )\n",
    "    return replay_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_algorithm(script_params, algo_hyperparams, seed=10, use_gpu=False):\n",
    "    env = script_params['env']\n",
    "    \n",
    "    env.seed(seed)\n",
    "    ptu.seed(seed)\n",
    "\n",
    "    ptu.set_gpu_mode(use_gpu)\n",
    "    \n",
    "    # Algorithm\n",
    "    algorithm = IUWeightedMultiSAC(\n",
    "        **script_params,\n",
    "        **algo_hyperparams,\n",
    "    )\n",
    "    \n",
    "    if ptu.gpu_enabled():\n",
    "        algorithm.cuda()\n",
    "    \n",
    "    return algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IU-SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "seed = 10\n",
    "Tend = 3.  # Seconds\n",
    "sim_timestep = 0.01\n",
    "frame_skip = 1\n",
    "goal = (0.65, 0.65)\n",
    "tgt_pose = (0.5, 0.25, 1.4660)\n",
    "\n",
    "env, env_params = create_environment(goal=goal, tgt_pose=tgt_pose, sim_timestep=sim_timestep, frame_skip=frame_skip, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common parameters\n",
    "n_unintentions = env.n_subgoals\n",
    "obs_dim = np.prod(env.observation_space.shape)\n",
    "action_dim = np.prod(env.action_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "net_size = 32\n",
    "u_qf, u_qf2, u_vf, i_qf, i_qf2, i_vf = create_value_fcns(net_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Hyperparameters\n",
    "policy_hyperparams = dict(\n",
    "net_size=32,\n",
    "n_shared_layers = 0,\n",
    "n_unshared_layers = 2,\n",
    "n_mix_layers = 2,\n",
    "shared_norm = False,\n",
    "unshared_norm = False,\n",
    "mix_norm = False, \n",
    ")\n",
    "policy = create_policy(**policy_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Buffer\n",
    "replay_buffer_size = 1e6\n",
    "replay_buffer = create_replay_buffer(env, replay_buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-24 16:40:43.330147 CEST | Variant:\n",
      "2018-09-24 16:40:43.330730 CEST | {\n",
      "  \"num_steps_per_epoch\": 1500,\n",
      "  \"num_epochs\": 1500,\n",
      "  \"num_updates_per_train_call\": 1,\n",
      "  \"num_steps_per_eval\": 900,\n",
      "  \"max_path_length\": 300,\n",
      "  \"render\": false,\n",
      "  \"min_steps_start_train\": 256,\n",
      "  \"min_start_eval\": 1500,\n",
      "  \"reparameterize\": true,\n",
      "  \"action_prior\": \"uniform\",\n",
      "  \"i_entropy_scale\": 1.0,\n",
      "  \"u_entropy_scale\": [\n",
      "    1.0,\n",
      "    1.0\n",
      "  ],\n",
      "  \"discount\": 0.99,\n",
      "  \"reward_scale\": 0.7,\n",
      "  \"u_reward_scales\": [\n",
      "    0.7,\n",
      "    0.7\n",
      "  ],\n",
      "  \"log_tensorboard\": false,\n",
      "  \"env_params\": {\n",
      "    \"is_render\": false,\n",
      "    \"obs_with_img\": false,\n",
      "    \"goal_poses\": \"[(0.65, 0.65), (0.65, 'any'), ('any', 0.65)]\",\n",
      "    \"rdn_goal_pose\": true,\n",
      "    \"tgt_pose\": [\n",
      "      0.5,\n",
      "      0.25,\n",
      "      1.466\n",
      "    ],\n",
      "    \"rdn_tgt_object_pose\": true,\n",
      "    \"sim_timestep\": 0.01,\n",
      "    \"frame_skip\": 1,\n",
      "    \"obs_distances\": true,\n",
      "    \"tgt_cost_weight\": 1.0,\n",
      "    \"goal_cost_weight\": 1.5,\n",
      "    \"ctrl_cost_weight\": 0.0001,\n",
      "    \"use_log_distances\": true,\n",
      "    \"log_alpha\": 0.1,\n",
      "    \"max_time\": null,\n",
      "    \"subtask\": null,\n",
      "    \"seed\": 10\n",
      "  }\n",
      "}\n",
      "Log directory is: /home/desteban/logs/notebook-2d3dofpusher-IUWeightedMultiSAC/notebook_2d3dofpusher_IUWeightedMultiSAC_2018_09_24_16_40_43_0000--s-0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'is_render': False,\n",
       " 'obs_with_img': False,\n",
       " 'goal_poses': [(0.65, 0.65), (0.65, 'any'), ('any', 0.65)],\n",
       " 'rdn_goal_pose': True,\n",
       " 'tgt_pose': (0.5, 0.25, 1.466),\n",
       " 'rdn_tgt_object_pose': True,\n",
       " 'sim_timestep': 0.01,\n",
       " 'frame_skip': 1,\n",
       " 'obs_distances': True,\n",
       " 'tgt_cost_weight': 1.0,\n",
       " 'goal_cost_weight': 1.5,\n",
       " 'ctrl_cost_weight': 0.0001,\n",
       " 'use_log_distances': True,\n",
       " 'log_alpha': 0.1,\n",
       " 'max_time': None,\n",
       " 'subtask': None,\n",
       " 'seed': 10}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------- #\n",
    "# Algorithm #\n",
    "# --------- #\n",
    "\n",
    "# Notebook Hypeparameters\n",
    "render = False\n",
    "n_epochs = 1500\n",
    "batch_size=256\n",
    "\n",
    "reward_scale=7.0e-1\n",
    "u_reward_scales=[7.0e-1, 7.0e-1]\n",
    "\n",
    "i_entropy_scale=1.0e-0\n",
    "u_entropy_scale=[1.0e-0, 1.0e-0]\n",
    "\n",
    "paths_per_epoch = 5\n",
    "paths_per_eval = 3\n",
    "dt = sim_timestep * frame_skip\n",
    "path_length = int(np.ceil(Tend/dt))\n",
    "\n",
    "log_tensorboard = False\n",
    "\n",
    "# ALGORITHM HYPERPARAMETERS\n",
    "algo_hyperparams = dict(\n",
    "    # Common RL algorithm params\n",
    "    num_steps_per_epoch=paths_per_epoch * path_length,\n",
    "    num_epochs=n_epochs,\n",
    "    num_updates_per_train_call=1,\n",
    "    num_steps_per_eval=paths_per_eval * path_length,\n",
    "    # EnvSampler params\n",
    "    max_path_length=path_length,\n",
    "    render=render,\n",
    "    # SAC params\n",
    "    min_steps_start_train=batch_size,\n",
    "    min_start_eval=paths_per_epoch * path_length,\n",
    "    reparameterize=True,\n",
    "    action_prior='uniform',\n",
    "    i_entropy_scale=i_entropy_scale,\n",
    "    u_entropy_scale=u_entropy_scale,\n",
    "    \n",
    "    discount=0.99,\n",
    "    reward_scale=reward_scale,\n",
    "    u_reward_scales=u_reward_scales,\n",
    "    \n",
    "    log_tensorboard=log_tensorboard,\n",
    ")\n",
    "\n",
    "# SCRIPT HYPERPARAMETERS\n",
    "script_params = dict(\n",
    "    env=env,\n",
    "    policy=policy,\n",
    "    u_qf=u_qf,\n",
    "    u_vf=u_vf,\n",
    "    replay_buffer=replay_buffer,\n",
    "    batch_size=batch_size,\n",
    "    i_qf=i_qf,\n",
    "    i_vf=i_vf,\n",
    "    u_qf2=u_qf2,\n",
    "    i_qf2=i_qf2,\n",
    "    eval_env=env,\n",
    "    save_environment=False,\n",
    ")\n",
    "\n",
    "# Add env_hyperparams temporally\n",
    "algo_hyperparams['env_params'] = env_params\n",
    "\n",
    "# Logger\n",
    "log_dir = setup_logger(\n",
    "#     'notebook_2d_nav_'+str(type(algorithm).__name__),\n",
    "    'notebook_2d3dofpusher_'+str(IUWeightedMultiSAC.__name__),\n",
    "    variant=algo_hyperparams,\n",
    "    snapshot_mode='gap_and_last',\n",
    "    snapshot_gap=25,\n",
    "    log_dir=None,\n",
    "    log_stdout=False,\n",
    ")\n",
    "print('Log directory is:', log_dir)\n",
    "\n",
    "# Now popup env_params\n",
    "algo_hyperparams.pop('env_params')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "\n",
    "use_gpu = True\n",
    "\n",
    "\n",
    "algorithm = create_algorithm(script_params, algo_hyperparams, seed=seed, use_gpu=use_gpu)\n",
    "\n",
    "algorithm.train(start_epoch=start_epoch, train_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "deterministic = True\n",
    "subpolicy = 1 # None or int\n",
    "print('Max path length:', path_length)\n",
    "\n",
    "env.close()\n",
    "obs = env.reset()\n",
    "for t in range(path_length):\n",
    "    env.render()\n",
    "    action, pol_info = policy.get_action(obs, pol_idx=subpolicy)\n",
    "    obs, reward, done, env_info = env.step(action)\n",
    "#     print('obs:', obs, '| goal:', env.wrapped_env.goal_position, ' | reward:', reward)\n",
    "#     print('---')\n",
    "    if done:\n",
    "        print('Environment done!')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_v_fcn():\n",
    "    xlim = (-7, 7)\n",
    "    ylim = (-7, 7)\n",
    "    delta = 0.01\n",
    "    x_min, x_max = tuple(1.1 * np.array(xlim))\n",
    "    y_min, y_max = tuple(1.1 * np.array(ylim))\n",
    "    all_x = np.arange(x_min, x_max, delta)\n",
    "    all_y = np.arange(y_min, y_max, delta)\n",
    "    xy_mesh = np.meshgrid(all_x, all_y)\n",
    "    all_obs = np.array(xy_mesh).T.reshape(-1, 2)\n",
    "    \n",
    "    def plot_v_contours(ax, values):\n",
    "        values = values.reshape(len(all_x), len(all_y))\n",
    "\n",
    "        contours = ax.contour(xy_mesh[0], xy_mesh[1], values, 20,\n",
    "                          colors='dimgray')\n",
    "        ax.clabel(contours, inline=1, fontsize=10, fmt='%.0f')\n",
    "        ax.imshow(values, extent=(x_min, x_max, y_min, y_max), origin='lower',\n",
    "                  alpha=0.5)\n",
    "        \n",
    "        ax.set_xlim(xlim)\n",
    "        ax.set_ylim(ylim)\n",
    "        \n",
    "        ax.set_xlabel('X')\n",
    "        ax.set_ylabel('Y')\n",
    "        ax.axis('equal')\n",
    "        ax.set_aspect('equal', 'box')\n",
    "    \n",
    "    # Compute and plot Main Task Value-fcn\n",
    "    values, _ = i_vf.get_values(all_obs)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ax.set_title('V-function Main Task')\n",
    "    plot_v_contours(ax, values)\n",
    "    \n",
    "    # Compute and plot Sub-tasks Value-fcn\n",
    "    n_cols = 2\n",
    "    n_rows = int(np.ceil(n_unintentions/n_cols))\n",
    "    subgoals_fig, subgoals_axs = plt.subplots(n_rows, n_cols)\n",
    "    \n",
    "    subgoals_axs = np.atleast_2d(subgoals_axs)\n",
    "    \n",
    "    \n",
    "    for aa in range(n_unintentions):\n",
    "        row = aa // n_cols\n",
    "        col = aa % n_cols\n",
    "        subgo_ax = subgoals_axs[row, col]\n",
    "        values, _ = u_vf.get_values(all_obs, val_idxs=[aa])\n",
    "        values = values[0]\n",
    "        \n",
    "        subgo_ax.set_title('V-function Sub-Task %02d' % aa)\n",
    "        plot_v_contours(subgo_ax, values)\n",
    "        \n",
    "    \n",
    "plot_v_fcn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_q_fcn(obs):\n",
    "    delta = 0.01\n",
    "    x_min, y_min = env.action_space.low\n",
    "    x_max, y_max = env.action_space.high\n",
    "    xlim = (x_min, x_max)\n",
    "    ylim = (y_min, y_max)\n",
    "    all_x = np.arange(x_min, x_max, delta)\n",
    "    all_y = np.arange(y_min, y_max, delta)\n",
    "    xy_mesh = np.meshgrid(all_x, all_y)\n",
    "\n",
    "    all_acts = np.zeros((len(all_x)*len(all_y), 2))\n",
    "    all_acts[:, 0] = xy_mesh[0].ravel()\n",
    "    all_acts[:, 1] = xy_mesh[1].ravel()\n",
    "     \n",
    "    all_obs = np.broadcast_to(obs, (all_acts.shape[0], 2))\n",
    "    \n",
    "    def plot_q_contours(ax, q_values):\n",
    "        q_values = q_values.reshape(len(all_x), len(all_y))\n",
    "\n",
    "        contours = ax.contour(xy_mesh[0], xy_mesh[1], q_values, 20,\n",
    "                          colors='dimgray')\n",
    "        ax.clabel(contours, inline=1, fontsize=10, fmt='%.0f')\n",
    "        ax.imshow(q_values, extent=(x_min, x_max, y_min, y_max), origin='lower',\n",
    "                  alpha=0.5)\n",
    "        \n",
    "        ax.set_xlim(xlim)\n",
    "        ax.set_ylim(ylim)\n",
    "        \n",
    "        ax.set_xlabel('X')\n",
    "        ax.set_ylabel('Y')\n",
    "        ax.axis('equal')\n",
    "        ax.set_aspect('equal', 'box')\n",
    "    \n",
    "    def plot_action_samples(ax, actions):\n",
    "        x, y = actions[:, 0], actions[:, 1]\n",
    "        ax.scatter(x, y, c='b', marker='*')\n",
    "        ax.set_xlim(xlim)\n",
    "        ax.set_ylim(ylim)\n",
    "   \n",
    "    fig, all_axs = plt.subplots(1, n_unintentions + 1)\n",
    "    fig.suptitle('Observation: ' +  str(obs))\n",
    "\n",
    "    # Compute and plot Main Task State Value-fcn\n",
    "    all_axs[0].set_title('Main Task')\n",
    "    q_values, _ = i_qf.get_values(all_obs, all_acts)\n",
    "    \n",
    "    plot_q_contours(all_axs[0], q_values)\n",
    "    \n",
    "    # Compute and plot Main Task State-Action Value-fcn\n",
    "    action_samples, _ = policy.get_actions(all_obs[:50, :], pol_idx=None)\n",
    "    plot_action_samples(all_axs[0], action_samples)\n",
    "    \n",
    "    for aa in range(n_unintentions):\n",
    "        subgo_ax = all_axs[aa + 1]\n",
    "        subgo_ax.set_title('Sub-Task %02d' % aa)\n",
    "        \n",
    "        q_values, _ = u_qf.get_values(all_obs, all_acts, val_idxs=[aa])\n",
    "        q_values = q_values[0]        \n",
    "        plot_q_contours(subgo_ax, q_values)\n",
    "        \n",
    "        action_samples, _ = policy.get_actions(all_obs[:20, :], pol_idx=aa)\n",
    "        plot_action_samples(subgo_ax, action_samples)       \n",
    "\n",
    "all_obs = [\n",
    "    [5., 5.],\n",
    "    [-4., 5.],\n",
    "    [5., -4.],\n",
    "    [-4., -4.],\n",
    "    [0., 0.],\n",
    "]\n",
    "\n",
    "for obs in all_obs:\n",
    "    plot_q_fcn(obs)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "snapshot_dir = logger.get_snapshot_dir()\n",
    "\n",
    "data_file = os.path.join(snapshot_dir, 'progress.csv')\n",
    "\n",
    "def plot_data(csv_file, label='Policy Entropy', plot_intentional=True):\n",
    "    labels_to_plot = list()\n",
    "    \n",
    "    for uu in range(n_unintentions):\n",
    "        labels_to_plot.append(('[U-%02d] ' % uu) + label)\n",
    "        \n",
    "    if plot_intentional:\n",
    "        n_subplots = n_unintentions + 1\n",
    "        labels_to_plot.append('[I] ' + label)\n",
    "    else:\n",
    "        n_subplots = n_unintentions\n",
    "\n",
    "    data = get_csv_data(csv_file, labels_to_plot)\n",
    "\n",
    "    fig, axs = subplots(n_subplots)\n",
    "    if not isinstance(axs, np.ndarray):\n",
    "        axs = np.array([axs])\n",
    "    fig.subplots_adjust(hspace=0)\n",
    "    fig.suptitle(label,\n",
    "                 fontweight='bold')\n",
    "\n",
    "    for aa, ax in enumerate(axs):\n",
    "        ax.plot(data[aa])\n",
    "        if aa < n_unintentions:\n",
    "            ylabel = 'Un-%02d' % uu\n",
    "        else:\n",
    "            ylabel = 'In'\n",
    "        ax.set_ylabel(ylabel)\n",
    "        plt.setp(ax.get_xticklabels(), visible=False)\n",
    "\n",
    "    axs[-1].set_xlabel('Episodes')\n",
    "    plt.setp(axs[-1].get_xticklabels(), visible=True)\n",
    "\n",
    "    \n",
    "plot_data(data_file, label='Test Returns Mean')\n",
    "print('--')\n",
    "plot_data(data_file, label='Test Rewards Mean')\n",
    "print('--')\n",
    "plot_data(data_file, label='Policy Entropy')\n",
    "print('--')\n",
    "plot_data(data_file, label='Mixing Weights', plot_intentional=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
