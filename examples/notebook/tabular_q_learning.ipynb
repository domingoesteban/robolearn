{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "intro_1",
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "# Lab 1: Markov Decision Processes - Problem 3\n",
    "\n",
    "\n",
    "## Lab Instructions\n",
    "All your answers should be written in this notebook.  You shouldn't need to write or modify any other files.\n",
    "\n",
    "**You should execute every block of code to not miss any dependency.**\n",
    "\n",
    "*This project was developed by Peter Chen, Rocky Duan, Pieter Abbeel for the Berkeley Deep RL Bootcamp, August 2017. Bootcamp website with slides and lecture videos: https://sites.google.com/view/deep-rl-bootcamp/. It is adapted from CS188 project materials: http://ai.berkeley.edu/project_overview.html.*\n",
    "\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, numpy.random as nr, gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Sampling-based Tabular Q-Learning\n",
    "\n",
    "So far we have implemented Value Iteration and Policy Iteration, both of which require access to an MDP's dynamics model. This requirement can sometimes be restrictive - for example, if the environment is given as a blackbox physics simulator, then we won't be able to read off the whole transition model.\n",
    "\n",
    "We can however use sampling-based Q-Learning to learn from this type of environments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we will learn to control a Crawler robot. Let's first try some completely random actions to see how the robot moves and familiarize ourselves with Gym environment interface again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We can inspect the observation space and action space of this Gym Environment\n",
      "-----------------------------------------------------------------------------\n",
      "Action space: Discrete(4)\n",
      "It's a discrete space with 4 actions to take\n",
      "Each action corresponds to increasing/decreasing the angle of one of the joints\n",
      "We can also sample from this action space: 0\n",
      "Another action sample: 3\n",
      "Another action sample: 1\n",
      "Observation space: Tuple(Discrete(9), Discrete(13)) , which means it's a 9x13 grid.\n",
      "It's the discretized version of the robot's two joint angles\n"
     ]
    }
   ],
   "source": [
    "from robolearn.envs.crawler.crawler_env import CrawlingRobotEnv\n",
    "\n",
    "env = CrawlingRobotEnv()\n",
    "\n",
    "print(\"We can inspect the observation space and action space of this Gym Environment\")\n",
    "print(\"-----------------------------------------------------------------------------\")\n",
    "print(\"Action space:\", env.action_space)\n",
    "print(\"It's a discrete space with %i actions to take\" % env.action_space.n)\n",
    "print(\"Each action corresponds to increasing/decreasing the angle of one of the joints\")\n",
    "print(\"We can also sample from this action space:\", env.action_space.sample())\n",
    "print(\"Another action sample:\", env.action_space.sample())\n",
    "print(\"Another action sample:\", env.action_space.sample())\n",
    "print(\"Observation space:\", env.observation_space, \", which means it's a 9x13 grid.\")\n",
    "print(\"It's the discretized version of the robot's two joint angles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = CrawlingRobotEnv(\n",
    "    render=True, # turn render mode on to visualize random motion\n",
    ")\n",
    "\n",
    "# standard procedure for interfacing with a Gym environment\n",
    "cur_state = env.reset() # reset environment and get initial state\n",
    "ret = 0.\n",
    "done = False\n",
    "i = 0\n",
    "while not done:\n",
    "    action = env.action_space.sample() # sample an action randomly\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    ret += reward\n",
    "    cur_state = next_state\n",
    "    i += 1\n",
    "    if i == 1500:\n",
    "        break # for the purpose of this visualization, let's only run for 1500 steps\n",
    "        # also note the GUI won't close automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you can close the visualization GUI with the following method \n",
    "env.close_gui()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see the random controller can sometimes make progress but it won't get very far. Let's implement Tabular Q-Learning with $\\epsilon$-greedy exploration to find a better policy piece by piece.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values for state (0, 0): [ 0.  0.  0.  0.] which is a list of Q values for each action\n",
      "As such, the Q value of taking action 3 in state (1,2), i.e. Q((1,2), 3), can be accessed by q_vals[(1,2)][3]: 0.0\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# dictionary that maps from state, s, to a numpy array of Q values [Q(s, a_1), Q(s, a_2) ... Q(s, a_n)]\n",
    "#   and everything is initialized to 0.\n",
    "q_vals = defaultdict(lambda: np.array([0. for _ in range(env.action_space.n)]))\n",
    "\n",
    "print(\"Q-values for state (0, 0): %s\" % q_vals[(0, 0)], \"which is a list of Q values for each action\")\n",
    "print(\"As such, the Q value of taking action 3 in state (1,2), i.e. Q((1,2), 3), can be accessed by q_vals[(1,2)][3]:\", q_vals[(1,2)][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test1 passed\n",
      "Test2 passed\n"
     ]
    }
   ],
   "source": [
    "def eps_greedy(q_vals, eps, state):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        q_vals: q value tables\n",
    "        eps: epsilon\n",
    "        state: current state\n",
    "    Outputs:\n",
    "        random action with probability of eps; argmax Q(s, .) with probability of (1-eps)\n",
    "    \"\"\"\n",
    "    # you might want to use random.random() to implement random exploration\n",
    "    #   number of actions can be read off from len(q_vals[state])\n",
    "    import random\n",
    "    # YOUR CODE HERE\n",
    "    random_num = random.random()\n",
    "    if random_num < eps:\n",
    "        act = random.randrange(len(q_vals[state]))\n",
    "    else:\n",
    "        act = np.argmax(q_vals[state])\n",
    "    return act\n",
    "    \n",
    "# test 1\n",
    "dummy_q = defaultdict(lambda: np.array([0. for _ in range(env.action_space.n)]))\n",
    "test_state = (0, 0)\n",
    "dummy_q[test_state][0] = 10.\n",
    "trials = 100000\n",
    "sampled_actions = [\n",
    "    int(eps_greedy(dummy_q, 0.3, test_state))\n",
    "    for _ in range(trials)\n",
    "]\n",
    "freq = np.sum(np.array(sampled_actions) == 0) / trials\n",
    "tgt_freq = 0.3 / env.action_space.n + 0.7\n",
    "if np.isclose(freq, tgt_freq, atol=1e-2):\n",
    "    print(\"Test1 passed\")\n",
    "else:\n",
    "    print(\"Test1: Expected to select 0 with frequency %.2f but got %.2f\" % (tgt_freq, freq))\n",
    "    \n",
    "# test 2\n",
    "dummy_q = defaultdict(lambda: np.array([0. for _ in range(env.action_space.n)]))\n",
    "test_state = (0, 0)\n",
    "dummy_q[test_state][2] = 10.\n",
    "trials = 100000\n",
    "sampled_actions = [\n",
    "    int(eps_greedy(dummy_q, 0.5, test_state))\n",
    "    for _ in range(trials)\n",
    "]\n",
    "freq = np.sum(np.array(sampled_actions) == 2) / trials\n",
    "tgt_freq = 0.5 / env.action_space.n + 0.5\n",
    "if np.isclose(freq, tgt_freq, atol=1e-2):\n",
    "    print(\"Test2 passed\")\n",
    "else:\n",
    "    print(\"Test2: Expected to select 2 with frequency %.2f but got %.2f\" % (tgt_freq, freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will implement Q learning update. After we observe a transition $s, a, s', r$,\n",
    "\n",
    "$$\\textrm{target}(s') = R(s,a,s') + \\gamma \\max_{a'} Q_{\\theta_k}(s',a')$$\n",
    "\n",
    "\n",
    "$$Q_{k+1}(s,a) \\leftarrow (1-\\alpha) Q_k(s,a) + \\alpha \\left[ \\textrm{target}(s') \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed\n"
     ]
    }
   ],
   "source": [
    "def q_learning_update(gamma, alpha, q_vals, cur_state, action, next_state, reward):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        gamma: discount factor\n",
    "        alpha: learning rate\n",
    "        q_vals: q value table\n",
    "        cur_state: current state\n",
    "        action: action taken in current state\n",
    "        next_state: next state results from taking `action` in `cur_state`\n",
    "        reward: reward received from this transition\n",
    "    \n",
    "    Performs in-place update of q_vals table to implement one step of Q-learning\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    target = reward + gamma * np.max(q_vals[next_state])\n",
    "    q_vals[cur_state][action] = (1-alpha)*q_vals[cur_state][action] + alpha*target\n",
    "\n",
    "# testing your q_learning_update implementation\n",
    "dummy_q = q_vals.copy()\n",
    "test_state = (0, 0)\n",
    "test_next_state = (0, 1)\n",
    "dummy_q[test_state][0] = 10.\n",
    "dummy_q[test_next_state][1] = 10.\n",
    "q_learning_update(0.9, 0.1, dummy_q, test_state, 0, test_next_state, 1.1)\n",
    "tgt = 10.01\n",
    "if np.isclose(dummy_q[test_state][0], tgt,):\n",
    "    print(\"Test passed\")\n",
    "else:\n",
    "    print(\"Q(test_state, 0) is expected to be %.2f but got %.2f\" % (tgt, dummy_q[test_state][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itr 0 # Average speed: 0.13\n",
      "Itr 5 # Average speed: 0.52\n",
      "Itr 10 # Average speed: 0.46\n",
      "Itr 15 # Average speed: 0.52\n",
      "Itr 20 # Average speed: 0.52\n",
      "Itr 25 # Average speed: 0.52\n",
      "Itr 30 # Average speed: 0.52\n",
      "Itr 35 # Average speed: 0.52\n",
      "Itr 40 # Average speed: 0.52\n",
      "Itr 45 # Average speed: 0.52\n",
      "Itr 50 # Average speed: 0.52\n",
      "Itr 55 # Average speed: 0.52\n",
      "Itr 60 # Average speed: 1.54\n",
      "Itr 65 # Average speed: 1.54\n",
      "Itr 70 # Average speed: 1.54\n",
      "Itr 75 # Average speed: 1.54\n",
      "Itr 80 # Average speed: 1.54\n",
      "Itr 85 # Average speed: 1.54\n",
      "Itr 90 # Average speed: 1.54\n",
      "Itr 95 # Average speed: 1.54\n",
      "Itr 100 # Average speed: 1.54\n",
      "Itr 105 # Average speed: 2.03\n",
      "Itr 110 # Average speed: 2.03\n",
      "Itr 115 # Average speed: 2.03\n",
      "Itr 120 # Average speed: 2.03\n",
      "Itr 125 # Average speed: 2.03\n",
      "Itr 130 # Average speed: 2.03\n",
      "Itr 135 # Average speed: 2.03\n",
      "Itr 140 # Average speed: 2.03\n",
      "Itr 145 # Average speed: 2.03\n",
      "Itr 150 # Average speed: 2.03\n",
      "Itr 155 # Average speed: 2.03\n",
      "Itr 160 # Average speed: 2.03\n",
      "Itr 165 # Average speed: 2.03\n",
      "Itr 170 # Average speed: 2.03\n",
      "Itr 175 # Average speed: 2.03\n",
      "Itr 180 # Average speed: 2.03\n",
      "Itr 185 # Average speed: 2.03\n",
      "Itr 190 # Average speed: 2.03\n",
      "Itr 195 # Average speed: 2.03\n",
      "Itr 200 # Average speed: 2.03\n",
      "Itr 205 # Average speed: 2.03\n",
      "Itr 210 # Average speed: 2.03\n",
      "Itr 215 # Average speed: 2.03\n",
      "Itr 220 # Average speed: 2.03\n",
      "Itr 225 # Average speed: 2.03\n",
      "Itr 230 # Average speed: 2.03\n",
      "Itr 235 # Average speed: 2.03\n",
      "Itr 240 # Average speed: 2.03\n",
      "Itr 245 # Average speed: 2.03\n",
      "Itr 250 # Average speed: 2.03\n",
      "Itr 255 # Average speed: 2.03\n",
      "Itr 260 # Average speed: 2.03\n",
      "Itr 265 # Average speed: 2.03\n",
      "Itr 270 # Average speed: 2.03\n",
      "Itr 275 # Average speed: 2.03\n",
      "Itr 280 # Average speed: 2.03\n",
      "Itr 285 # Average speed: 2.03\n",
      "Itr 290 # Average speed: 2.03\n",
      "Itr 295 # Average speed: 2.03\n",
      "Itr 300 # Average speed: 2.03\n",
      "Itr 305 # Average speed: 2.03\n",
      "Itr 310 # Average speed: 2.89\n",
      "Itr 315 # Average speed: 2.89\n",
      "Itr 320 # Average speed: 3.28\n",
      "Itr 325 # Average speed: 3.28\n",
      "Itr 330 # Average speed: 3.28\n",
      "Itr 335 # Average speed: 3.28\n",
      "Itr 340 # Average speed: 3.28\n",
      "Itr 345 # Average speed: 3.28\n",
      "Itr 350 # Average speed: 3.28\n",
      "Itr 355 # Average speed: 3.28\n",
      "Itr 360 # Average speed: 3.28\n",
      "Itr 365 # Average speed: 3.28\n",
      "Itr 370 # Average speed: 3.28\n",
      "Itr 375 # Average speed: 3.28\n",
      "Itr 380 # Average speed: 3.28\n",
      "Itr 385 # Average speed: 3.28\n",
      "Itr 390 # Average speed: 3.28\n",
      "Itr 395 # Average speed: 3.28\n",
      "Itr 400 # Average speed: 3.28\n",
      "Itr 405 # Average speed: 3.28\n",
      "Itr 410 # Average speed: 3.28\n",
      "Itr 415 # Average speed: 3.28\n",
      "Itr 420 # Average speed: 3.28\n",
      "Itr 425 # Average speed: 3.28\n",
      "Itr 430 # Average speed: 3.28\n",
      "Itr 435 # Average speed: 3.28\n",
      "Itr 440 # Average speed: 3.28\n",
      "Itr 445 # Average speed: 3.28\n",
      "Itr 450 # Average speed: 3.37\n",
      "Itr 455 # Average speed: 3.37\n",
      "Itr 460 # Average speed: 3.37\n",
      "Itr 465 # Average speed: 3.37\n",
      "Itr 470 # Average speed: 3.37\n",
      "Itr 475 # Average speed: 3.37\n",
      "Itr 480 # Average speed: 3.37\n",
      "Itr 485 # Average speed: 3.37\n",
      "Itr 490 # Average speed: 3.37\n",
      "Itr 495 # Average speed: 3.37\n",
      "Itr 500 # Average speed: 3.37\n",
      "Itr 505 # Average speed: 3.37\n",
      "Itr 510 # Average speed: 3.37\n",
      "Itr 515 # Average speed: 3.37\n",
      "Itr 520 # Average speed: 3.37\n",
      "Itr 525 # Average speed: 3.37\n",
      "Itr 530 # Average speed: 3.37\n",
      "Itr 535 # Average speed: 3.37\n",
      "Itr 540 # Average speed: 3.37\n",
      "Itr 545 # Average speed: 3.37\n",
      "Itr 550 # Average speed: 3.37\n",
      "Itr 555 # Average speed: 3.37\n",
      "Itr 560 # Average speed: 3.37\n",
      "Itr 565 # Average speed: 3.37\n",
      "Itr 570 # Average speed: 3.37\n",
      "Itr 575 # Average speed: 3.37\n",
      "Itr 580 # Average speed: 3.37\n",
      "Itr 585 # Average speed: 3.37\n",
      "Itr 590 # Average speed: 3.37\n",
      "Itr 595 # Average speed: 3.37\n",
      "Itr 600 # Average speed: 3.37\n",
      "Itr 605 # Average speed: 3.37\n",
      "Itr 610 # Average speed: 3.37\n",
      "Itr 615 # Average speed: 3.37\n",
      "Itr 620 # Average speed: 3.37\n",
      "Itr 625 # Average speed: 3.37\n",
      "Itr 630 # Average speed: 3.37\n",
      "Itr 635 # Average speed: 3.37\n",
      "Itr 640 # Average speed: 3.37\n",
      "Itr 645 # Average speed: 3.37\n",
      "Itr 650 # Average speed: 3.37\n",
      "Itr 655 # Average speed: 3.37\n",
      "Itr 660 # Average speed: 3.37\n",
      "Itr 665 # Average speed: 3.37\n",
      "Itr 670 # Average speed: 3.37\n",
      "Itr 675 # Average speed: 3.37\n",
      "Itr 680 # Average speed: 3.37\n",
      "Itr 685 # Average speed: 3.37\n",
      "Itr 690 # Average speed: 3.37\n",
      "Itr 695 # Average speed: 3.37\n",
      "Itr 700 # Average speed: 3.37\n",
      "Itr 705 # Average speed: 3.37\n",
      "Itr 710 # Average speed: 3.37\n",
      "Itr 715 # Average speed: 3.37\n",
      "Itr 720 # Average speed: 3.37\n",
      "Itr 725 # Average speed: 3.37\n",
      "Itr 730 # Average speed: 3.37\n",
      "Itr 735 # Average speed: 3.37\n",
      "Itr 740 # Average speed: 3.37\n",
      "Itr 745 # Average speed: 3.37\n",
      "Itr 750 # Average speed: 3.37\n",
      "Itr 755 # Average speed: 3.37\n",
      "Itr 760 # Average speed: 3.37\n",
      "Itr 765 # Average speed: 3.37\n",
      "Itr 770 # Average speed: 3.37\n",
      "Itr 775 # Average speed: 3.37\n",
      "Itr 780 # Average speed: 3.37\n",
      "Itr 785 # Average speed: 3.37\n",
      "Itr 790 # Average speed: 3.37\n",
      "Itr 795 # Average speed: 3.37\n",
      "Itr 800 # Average speed: 3.37\n",
      "Itr 805 # Average speed: 3.37\n",
      "Itr 810 # Average speed: 3.37\n",
      "Itr 815 # Average speed: 3.37\n",
      "Itr 820 # Average speed: 3.37\n",
      "Itr 825 # Average speed: 3.37\n",
      "Itr 830 # Average speed: 3.37\n",
      "Itr 835 # Average speed: 3.37\n",
      "Itr 840 # Average speed: 3.37\n",
      "Itr 845 # Average speed: 3.37\n",
      "Itr 850 # Average speed: 3.37\n",
      "Itr 855 # Average speed: 3.37\n",
      "Itr 860 # Average speed: 3.37\n",
      "Itr 865 # Average speed: 3.37\n",
      "Itr 870 # Average speed: 3.37\n",
      "Itr 875 # Average speed: 3.37\n",
      "Itr 880 # Average speed: 3.37\n",
      "Itr 885 # Average speed: 3.37\n",
      "Itr 890 # Average speed: 3.37\n",
      "Itr 895 # Average speed: 3.37\n",
      "Itr 900 # Average speed: 3.37\n",
      "Itr 905 # Average speed: 3.37\n",
      "Itr 910 # Average speed: 3.37\n",
      "Itr 915 # Average speed: 3.37\n",
      "Itr 920 # Average speed: 3.37\n",
      "Itr 925 # Average speed: 3.37\n",
      "Itr 930 # Average speed: 3.37\n",
      "Itr 935 # Average speed: 3.37\n",
      "Itr 940 # Average speed: 3.37\n",
      "Itr 945 # Average speed: 3.37\n",
      "Itr 950 # Average speed: 3.37\n",
      "Itr 955 # Average speed: 3.37\n",
      "Itr 960 # Average speed: 3.37\n",
      "Itr 965 # Average speed: 3.37\n",
      "Itr 970 # Average speed: 3.37\n",
      "Itr 975 # Average speed: 3.37\n",
      "Itr 980 # Average speed: 3.37\n",
      "Itr 985 # Average speed: 3.37\n",
      "Itr 990 # Average speed: 3.37\n",
      "Itr 995 # Average speed: 3.37\n"
     ]
    }
   ],
   "source": [
    "# now with the main components tested, we can put everything together to create a complete q learning agent\n",
    "\n",
    "env = CrawlingRobotEnv()\n",
    "q_vals = defaultdict(lambda: np.array([0. for _ in range(env.action_space.n)]))\n",
    "gamma = 0.9\n",
    "alpha = 0.1\n",
    "eps = 0.5\n",
    "cur_state = env.reset()\n",
    "\n",
    "def greedy_eval():\n",
    "    \"\"\"evaluate greedy policy w.r.t current q_vals\"\"\"\n",
    "    test_env = CrawlingRobotEnv(horizon=np.inf)\n",
    "    prev_state = test_env.reset()\n",
    "    ret = 0.\n",
    "    done = False\n",
    "    H = 100\n",
    "    for i in range(H):\n",
    "        action = np.argmax(q_vals[prev_state])\n",
    "        state, reward, done, info = test_env.step(action)\n",
    "        ret += reward\n",
    "        prev_state = state\n",
    "    return ret / H\n",
    "\n",
    "# for itr in range(300000):\n",
    "for itr in range(1000):\n",
    "    # YOUR CODE HERE\n",
    "    # Hint: use eps_greedy & q_learning_update\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    H = 100\n",
    "    ret = 0.\n",
    "    for t in range(H):\n",
    "        action = eps_greedy(q_vals, eps, state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        ret += reward\n",
    "        if done:\n",
    "            break\n",
    "        q_learning_update(gamma, alpha, q_vals, state, action, next_state, reward)\n",
    "        state = next_state\n",
    "    \n",
    "#     if itr % 50000 == 0: # evaluation\n",
    "    if itr % 5 == 0: # evaluation\n",
    "        print(\"Itr %i # Average speed: %.2f\" % (itr, greedy_eval()))\n",
    "\n",
    "# at the end of learning your crawler should reach a speed of >= 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the learning is successful, we can visualize the learned robot controller. Remember we learn this just from interacting with the environment instead of peeking into the dynamics model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = CrawlingRobotEnv(render=True, horizon=500)\n",
    "prev_state = env.reset()\n",
    "ret = 0.\n",
    "done = False\n",
    "while not done:\n",
    "    action = np.argmax(q_vals[prev_state])\n",
    "    state, reward, done, info = env.step(action)\n",
    "    ret += reward\n",
    "    prev_state = state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you can close the visualization GUI with the following method \n",
    "env.close_gui()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
