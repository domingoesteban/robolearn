# ############### #
# Task parameters #
# ############### #
#Ts: 0.03 # 0.01
Ts: 0.01 # 0.01
Tend: 1.0  # 0.7
#Tend: 0.1  # 0.7

# ############## #
# Env parameters #
# ############## #
render: False
goal_tolerance: 0.02

# ############################ #
# Initial condition parameters #
# ############################ #
fix_init_conds: 1  # 0 or 1 | If 1 use the specified init_cond else generate them randomly
# Fixed initial conditions  [q0, q1, 12, tgt1x, tgt1y, tgt1ori, tgt2x, tgt2y, tgt2ori]
# Fixed initial conditions  [tgtX, tgtY, tgtYAW, obstX, obstY, obstYAW]
init_cond: [
#[0.05, 0.4, 00.0, 0.08, 0.2, 0.0]
[0.02, 0.3, 00.0, 0.08, 0.6, 0.0],
[0.03, 0.1, 00.0, 0.08, 0.6, 0.0],
 ]

# Parameters for generation new initial conditions
mean_init_cond: [20, 15, 5, 0.60, -0.10] # des_tgt(using joint_config), obstacle (X,Y)
std_init_cond: [30, 30, 10, 0.1, 0.1]  # lim/2
init_joint_pos: [-90, 20, 2]  # Fixed initial joint config
n_init_cond: 15

train_cond: [0, 1] #, 2, 3]  # Condition idxs for training
test_cond: [0, 1] #, 2, 3, 4] # Condition idxs for testing

# ################ #
# Agent parameters #
# ################ #
tf_iterations: 5000 # Number of iterations per inner iteration
use_gpu: True
gpu_mem_percentage: 0.1
np_threads: 3

# ############### #
# Cost parameters #
# ############### #
#cost_weights: [1.0e-6,  # Action | w5
#               1.0e+0,  # State Difference | w1
#               1.0e+3,  # Final State Difference |w3
#               2.0e+3,  # Safe Distance | w2
#               2.0e+6]  # Final Safe Distance |w4
cost_weights: [1.0e+4,  # Action | w5
               1.0e+3,  # State Difference | w1
               1.0e+3,  # Final State Difference |w3
#               0.0e+0,  # Safe Distance | w2
#               0.0e+0,  # Final Safe Distance |w4
               ]

state_diff_weights: [
                     1.0, # X
                     1.0, # Y
                     1.0, # Z
                     1.0, # Roll
                     1.0, # Pitch
                     1.0, # Yaw
                    ]

l2_l1: [
        1.0e+0, # l2
        0.0e-1, # l1  # Mejor cero. Problemas con
       ]

inside_cost: 1.0

# #################### #
# Algorithm parameters #
# #################### #
# Interaction
iterations: 200
num_samples: 6 # Samples for exploration trajs --> N samples
sample_on_policy: False # Whether generate on-policy samples or off-policy samples
test_after_iter: True # If test the learned policy after an iteration in the RL algorithm
test_n_samples: 1  # Samples from learned policy after an iteration PER CONDITION (only if 'test_after_iter':True)

# SL step
forget_bad_samples: False  # Do not include the bad samples in SL
tf_seed: 10  # -1 if we want to use same seed defined in the main script

# TrajOpt parameters
inner_iterations: 1

# Dualist GPS parameters
consider_bad: False
consider_good: False

bad_costs: []  # [3, 4]  # Indexes to consider bad criteria (Sum of selected ones)

# TODO: CHANGEEEEEEEEEE THE KL_STEP
kl_step: 0.01 # epsilon
#kl_step: 1000000000000.1 # epsilon
kl_bad: 60.0 # xi
kl_good: 0.4 # chi

max_nu: 2.0e-0  # Bad dual variable
max_omega: 1.0e-1  # Good dual variable

weight_bad: 5.e-1  # It wont be used
weight_good: 1.e-1  # It wont be used

min_bad_rel_diff: 0.10
max_bad_rel_diff: 10.0
mult_bad_rel_diff: 1
good_fix_rel_multi: 5

n_bad_buffer: 20 # Number of bad samples per trajectory distribution
n_good_buffer: 20 # Number of good samples per trajectory distribution

n_bad_samples: 1 # Number of bad samples per trajectory distribution
n_good_samples: 1 # Number of good samples per trajectory distribution

bad_traj_selection_type: temp  # 'temp', 'cost' | temp: update every iteration
good_traj_selection_type: temp  # 'temp', 'cost' | cost: update with extreme sample costs
