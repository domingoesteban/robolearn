# ############### #
# Task parameters #
# ############### #
Ts: 0.03 # 0.01
Tend: 15  # 0.7

# ############## #
# Env parameters #
# ############## #
render: False
obs_like_mjc: False
ntargets: 2
tgt_weights: [1.0, -1.0]
rdn_tgt_pos: True
tgt_positions: [[0.1, 0.2], [-0.1, -0.2]]  # Values between [-0.2, 0.2]
tgt_types: ['CS', 'C']

# ############################ #
# Initial condition parameters #
# ############################ #
fix_init_conds: 1  # 0 or 1 | If 1 use the specified init_cond else generate them randomly
# Fixed initial conditions  [q0, q1, 12, tgt1x, tgt1y, tgt1ori, tgt2x, tgt2y, tgt2ori]
init_cond: [
 [-90.    ,  20.    ,   2.    ,   0.3693,   0.6511,  83.9477,   0.6545,  -0.0576,   0.    ],
 [-90.    ,  20.    ,   2.    ,   0.3913,   0.6548,  81.4222,   0.6964,  -0.0617,   0.    ],
 [-90.    ,  20.    ,   2.    ,   0.3296,   0.6863,  85.299 ,   0.6926,  -0.0929,   0.    ],
 [-90.    ,  20.    ,   2.    ,   0.6426,   0.4486,  51.5466,   0.6778,  -0.013 ,   0.    ],
 [-90.    ,  20.    ,   2.    ,   0.1991,   0.7172,  97.9481,   0.6781,  -0.0882,   0.    ]]

# Parameters for generation new initial conditions
mean_init_cond: [20, 15, 5, 0.60, -0.10] # des_tgt(using joint_config), obstacle (X,Y)
std_init_cond: [30, 30, 10, 0.1, 0.1]  # lim/2
init_joint_pos: [-90, 20, 2]  # Fixed initial joint config
n_init_cond: 15

train_cond: [0, 1, 2, 3]  # Condition idxs for training
test_cond: [0, 1, 2, 3, 4] # Condition idxs for testing

# ################ #
# Agent parameters #
# ################ #
tf_iterations: 5000 # 5000 # Number of iterations per inner iteration
use_gpu: True
gpu_mem_percentage: 0.1
np_threads: 3

# ############### #
# Cost parameters #
# ############### #
cost_weights: [1.0e-6,  # Action | w5
               1.0e+0,  # State Difference | w1
               1.0e+3,  # Final State Difference |w3
               2.0e+3,  # Safe Distance | w2
               2.0e+6]  # Final Safe Distance |w4

state_diff_weights: [
                     10.0, # X
                     10.0, # Y
                     0.1  # Theta
                    ]

l1_l2: [
        1.0e+1, # l1
        5.0e-2 # l2
       ]

inside_cost: 1.0

# #################### #
# Algorithm parameters #
# #################### #
# Interaction
iterations: 50
num_samples: 6 # Samples for exploration trajs --> N samples
sample_on_policy: False # Whether generate on-policy samples or off-policy samples
test_after_iter: True # If test the learned policy after an iteration in the RL algorithm
test_n_samples: 1  # Samples from learned policy after an iteration PER CONDITION (only if 'test_after_iter':True)

# SL step
forget_bad_samples: False  # Do not include the bad samples in SL
tf_seed: 0  # -1 if we want to use same seed defined in the main script

# TrajOpt parameters
inner_iterations: 1

# Dualist GPS parameters
consider_bad: True
consider_good: False

bad_costs: []  # [3, 4]  # Indexes to consider bad criteria (Sum of selected ones)

kl_step: 0.1 # epsilon
kl_bad: 60.0 # xi
kl_good: 0.4 # chi

max_nu: 2.0e-0  # Bad dual variable
max_omega: 1.0e-1  # Good dual variable

weight_bad: 5.e-1  # It wont be used
weight_good: 1.e-1  # It wont be used

min_bad_rel_diff: 0.10
max_bad_rel_diff: 10.0
mult_bad_rel_diff: 1
good_fix_rel_multi: 5

n_bad_buffer: 20 # Number of bad samples per trajectory distribution
n_good_buffer: 20 # Number of good samples per trajectory distribution

n_bad_samples: 1 # Number of bad samples per trajectory distribution
n_good_samples: 1 # Number of good samples per trajectory distribution

bad_traj_selection_type: temp  # 'temp', 'cost' | temp: update every iteration
good_traj_selection_type: temp  # 'temp', 'cost' | cost: update with extreme sample costs
