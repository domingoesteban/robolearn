learning_algorithm: 'MDREPS'

# Task parameters
Ts: 0.01 # 0.01
Tend: 10  # 0.7

# Env parameters
render: False
obs_like_mjc: False
ntargets: 2
tgt_weights: [1.0, -1.0]
rdn_tgt_pos: True
tgt_positions: [[0.1, 0.2], [-0.1, -0.2]]  # Values between [-0.2, 0.2]

# Initial conditions  [pos1, pos2, tgt1x, tgt1y, tgt2x, tgt2y]
init_cond: [
           [-90, 20, 2, 0.2, 0.5, 0.6, 0.0],
           [-90, 20, 3, 0.5, 0.0, 0.5, -0.5],
           [-90, 20, 2, 0.7, 0.3, 0.65, -0.2],
#           [-60, -20, 0, 0.7, 0.3, 0.7, 0.0],
#           [60, 20, 0, 0.7, 0.0, -0.6, 0.2],
#           [-60, -20, 0, 0.5, -0.1, -0.6, -0.2],
#           [45, 0, 0.2, 0.0, -0.1, 0.2],
           ]
train_cond: [0, 1]  # Indexes
test_cond: [2]

# Agent
tf_iterations: 5000 # 5000 # Number of iterations per inner iteration (Default:5000). Recommended: 1000?
use_gpu: True
gpu_mem_percentage: 0.2
np_threads: 5

# Learning interaction parameters
iterations: 50
num_samples: 10 # Samples for exploration trajs --> N samples
sample_on_policy: False # Whether generate on-policy samples or off-policy samples
test_after_iter: True # If test the learned policy after an iteration in the RL algorithm
test_n_samples: 1  # Samples from learned policy after an iteration PER CONDITION (only if 'test_after_iter':True)

# TrajOpt parameters
inner_iterations: 1

# Dualist GPS parameters
consider_good: False
consider_bad: False
