learning_algorithm: 'MDREPS'

# Task parameters
Ts: 0.01 # 0.01
Tend: 1 #15  # 0.7

# Env parameters
render: False
obs_like_mjc: False
ntargets: 2
tgt_weights: [1.0, -1.0]
rdn_tgt_pos: True
tgt_positions: [[0.1, 0.2], [-0.1, -0.2]]  # Values between [-0.2, 0.2]
tgt_types: ['CS', 'C']

# Initial conditions  [pos1...pos7, tgt1x, tgt1y, tgt1ori, tgt2x, tgt2y, tgt2ori]
init_cond: [
           [ 25, -40, -45,  -5,   0,   0,   0,  0.74,  -0.26,  35,  0.54, -0.43,  0],
           [ 20, -55,   0, -95,   0,   0,   0, 0.75,  -0.10,  10,  0.54, -0.43,  0], # FROM FILE?
           [ 20, -55,   0, -95,   0,   0,   0, 0.70,  -0.00,  10,  0.54, -0.43,  0],
           [-10, -30,   0, -95,   0,   0,   0, 0.74,  -0.26,  35,  0.54, -0.43,  0],
           [ 25, -40, -45,  -5,   0,   0,   0,  0.74,  0.26,  35,  0.54, -0.43,  0],
           ]
train_cond: [0]  # Indexes
test_cond: [0]

# Agent
tf_iterations: 5000 # 5000 # Number of iterations per inner iteration (Default:5000). Recommended: 1000?
use_gpu: True
gpu_mem_percentage: 0.1
np_threads: 1

# Learning interaction parameters
iterations: 50
num_samples: 10 # Samples for exploration trajs --> N samples
sample_on_policy: False # Whether generate on-policy samples or off-policy samples
test_after_iter: True # If test the learned policy after an iteration in the RL algorithm
test_n_samples: 1  # Samples from learned policy after an iteration PER CONDITION (only if 'test_after_iter':True)

# SL step
forget_bad_samples: False  # Do not include the bad samples in SL
tf_seed: 0  # -1 if we want to use same seed defined in the main script

# Cost
cost_weights: [1.0e-1,  # Action
               1.5e-1,  # FK L1
               1.0e-0,  # FK L2
               1.5e-1,  # Final FK L1
               1.0e-0,  # Final FK L2
               5.0e+0,  # State Distance
               1.0e+1]  # Final State Distance

# TrajOpt parameters
inner_iterations: 1

# Dualist GPS parameters
#consider_bad: True
#consider_good: True
#consider_bad: True
#consider_good: False
consider_bad: False
consider_good: False

bad_costs: []  # [3, 4]  # Indexes to consider bad criteria (Sum of selected ones)

kl_step: 0.2 # epsilon
kl_bad: 2.0 # xi
kl_good: 0.4 # chi


max_nu: 1.0e-1  # Bad dual variable
max_omega: 1.0e-2  # Good dual variable


weight_bad: 5.e-1
weight_good: 1.e-1


min_bad_rel_diff: 0.01
max_bad_rel_diff: 2.0
mult_bad_rel_diff: 1
good_fix_rel_multi: 5

n_bad_buffer: 10 # Number of bad samples per trajectory distribution
n_good_buffer: 10 # Number of good samples per trajectory distribution

n_bad_samples: 1 # Number of bad samples per trajectory distribution
n_good_samples: 1 # Number of good samples per trajectory distribution

#bad_traj_selection_type: cost  # 'temp', 'cost'
#good_traj_selection_type: cost  # 'temp', 'cost'
bad_traj_selection_type: temp  # 'temp', 'cost'
good_traj_selection_type: temp  # 'temp', 'cost'
